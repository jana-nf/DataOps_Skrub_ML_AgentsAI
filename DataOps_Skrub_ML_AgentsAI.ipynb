{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1cs39oo2igdvaussskrVPiIEc0fS3v1v6",
      "authorship_tag": "ABX9TyNiZtq5dzLEdV0VshuVKDim",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jana-nf/DataOps_Skrub_ML_AgentsAI/blob/main/DataOps_Skrub_ML_AgentsAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataOps: Integração de Agentes de IA e ML Clássico com Skrub, governança e auditabilidade Git"
      ],
      "metadata": {
        "id": "tI5fHKnG0cKV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuração Inicial e Instalação\n",
        "Como o Google Colab não vem com o skrub instalado por padrão, precisamos instalá-lo primeiro."
      ],
      "metadata": {
        "id": "EhwS2xJz1yw9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqL9gxck0aAq",
        "outputId": "14a4fb1a-67f1-4f08-d03f-16c14b640b54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting skrub\n",
            "  Downloading skrub-0.7.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.12/dist-packages (from skrub) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.9.3 in /usr/local/lib/python3.12/dist-packages (from skrub) (1.16.3)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from skrub) (3.1.6)\n",
            "Requirement already satisfied: matplotlib>=3.4.3 in /usr/local/lib/python3.12/dist-packages (from skrub) (3.10.0)\n",
            "Requirement already satisfied: requests>=2.27.1 in /usr/local/lib/python3.12/dist-packages (from skrub) (2.32.4)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.12/dist-packages (from skrub) (4.0.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=3.1.2->skrub) (3.0.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.4.3->skrub) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.4.3->skrub) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.4.3->skrub) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.4.3->skrub) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.4.3->skrub) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.4.3->skrub) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.4.3->skrub) (3.2.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.27.1->skrub) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.27.1->skrub) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.27.1->skrub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.27.1->skrub) (2025.11.12)\n",
            "Downloading skrub-0.7.0-py3-none-any.whl (498 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.3/498.3 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: skrub\n",
            "Successfully installed skrub-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install skrub scikit-learn pandas\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from skrub import TableVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuração, Montagem do Drive e Instalação\n",
        "Esta etapa conecta o Colab ao seu Google Drive"
      ],
      "metadata": {
        "id": "YzASOyvH2WPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yW9v3RXc2YIQ",
        "outputId": "3b37956f-c474-4972-b94e-87069fd100d9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Carregamento do Dataset do Drive e Limpeza Inicial"
      ],
      "metadata": {
        "id": "tmgj4vsm24yL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/Colab Notebooks/DataOps_Skrub_ML_AgentsAI/WA_Fn-UseC_-Telco-Customer-Churn.csv'\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"Dataset Telco Customer Churn carregado com sucesso do Google Drive!\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERRO: Arquivo não encontrado no caminho: {file_path}\")\n",
        "    print(\"Verifique se o nome do arquivo e o caminho no seu Drive estão corretos.\")\n",
        "    exit()\n",
        "\n",
        "# 1. Pré-limpeza crucial: O campo 'TotalCharges'\n",
        "# Converte a coluna 'TotalCharges' (que está como string com espaços) para float.\n",
        "df['TotalCharges'] = df['TotalCharges'].replace(' ', np.nan).astype(float)\n",
        "\n",
        "# 2. Separar Features (X) e Target (y)\n",
        "X = df.drop(columns=['Churn', 'customerID'])\n",
        "y = df['Churn'].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "# 3. Divisão Treino/Teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"\\n--- Informações do Dataset ---\")\n",
        "print(f\"Shape de Treino: {X_train.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZ57yDyC3xb8",
        "outputId": "e437baa5-c8da-4ae5-8385-745d62b5307d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Telco Customer Churn carregado com sucesso do Google Drive!\n",
            "\n",
            "--- Informações do Dataset ---\n",
            "Shape de Treino: (5634, 19)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aplicação do Skrub (TableVectorizer) no Pipeline\n",
        "Esta etapa encapsula o pré-processamento (Skrub) e o modelo de ML Clássico (Regressão Logística) no pipeline, demonstrando Clean Code e Integração."
      ],
      "metadata": {
        "id": "_p6Aa9sJ4Nma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from skrub import TableVectorizer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "numerical_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "# Criar um transformador para imputar *apenas* as colunas numéricas\n",
        "# O resto (as strings) será tratado pelo TableVectorizer.\n",
        "numerical_transformer = SimpleImputer(strategy='median')\n",
        "\n",
        "# O ColumnTransformer permite aplicar transformações seletivas.\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num_imputer', numerical_transformer, numerical_features),\n",
        "        ('passthrough', 'passthrough', X_train.columns.difference(numerical_features))\n",
        "    ],\n",
        "    remainder='drop',\n",
        "    verbose_feature_names_out=False\n",
        ")\n",
        "\n",
        "# Definir o Pipeline Final\n",
        "pipeline_ml_classico = make_pipeline(\n",
        "    preprocessor,\n",
        "    TableVectorizer(),\n",
        "    LogisticRegression(max_iter=1000, random_state=42)\n",
        ")\n",
        "\n",
        "# Treinar o Pipeline\n",
        "print(\"\\n--- Treinando o Pipeline Skrub + Regressão Logística (Corrigido) ---\")\n",
        "pipeline_ml_classico.fit(X_train, y_train)\n",
        "print(\"Treinamento concluído com sucesso!\")\n",
        "\n",
        "# Avaliar o Modelo Clássico (ML Clássico)\n",
        "y_pred = pipeline_ml_classico.predict(X_test)\n",
        "\n",
        "print(\"\\n--- Avaliação do ML Clássico (Base para o Agente de IA) ---\")\n",
        "print(classification_report(y_test, y_pred, target_names=['No Churn', 'Churn']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SP3X_TML6YO0",
        "outputId": "c442250e-88f2-400a-ea78-cc0edb8af01a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Treinando o Pipeline Skrub + Regressão Logística (Corrigido) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/skrub/_check_input.py:175: UserWarning: Only pandas and polars DataFrames are supported, but input is a Numpy array. Please convert Numpy arrays to DataFrames before passing them to skrub transformers. Converting to pandas DataFrame with columns ['0', '1', …].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/skrub/_clean_null_strings.py:39: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  return col.replace(r\"^\\s*$\", \"\", regex=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treinamento concluído com sucesso!\n",
            "\n",
            "--- Avaliação do ML Clássico (Base para o Agente de IA) ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    No Churn       0.85      0.89      0.87      1035\n",
            "       Churn       0.66      0.56      0.60       374\n",
            "\n",
            "    accuracy                           0.80      1409\n",
            "   macro avg       0.75      0.73      0.74      1409\n",
            "weighted avg       0.80      0.80      0.80      1409\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/skrub/_check_input.py:175: UserWarning: Only pandas and polars DataFrames are supported, but input is a Numpy array. Please convert Numpy arrays to DataFrames before passing them to skrub transformers. Converting to pandas DataFrame with columns ['0', '1', …].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/skrub/_clean_null_strings.py:39: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  return col.replace(r\"^\\s*$\", \"\", regex=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Geração de Valores SHAP (Interpretabilidade)\n",
        "Precisamos da biblioteca SHAP. Como estamos usando um Pipeline com TableVectorizer no início, usaremos um Explainer que pode lidar com transformações."
      ],
      "metadata": {
        "id": "g7aqz9f778qg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instalação"
      ],
      "metadata": {
        "id": "T7mqahkr8GYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap\n",
        "import shap\n",
        "import json\n",
        "import hashlib\n",
        "import time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FW_sOu2D7f4Y",
        "outputId": "f66c5734-39a8-4ccd-e2cc-29b43ece54cc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: shap in /usr/local/lib/python3.12/dist-packages (0.50.0)\n",
            "Requirement already satisfied: numpy>=2 in /usr/local/lib/python3.12/dist-packages (from shap) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from shap) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from shap) (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from shap) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.12/dist-packages (from shap) (4.67.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.12/dist-packages (from shap) (25.0)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.12/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.12/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from shap) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from shap) (4.15.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->shap) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->shap) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->shap) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Código SHAP"
      ],
      "metadata": {
        "id": "dhGj5bAq8R68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Acessar o modelo treinado (LogisticRegression)\n",
        "model = pipeline_ml_classico.named_steps['logisticregression']\n",
        "\n",
        "# Acessar o TableVectorizer treinado\n",
        "vectorizer = pipeline_ml_classico.named_steps['tablevectorizer']\n",
        "\n",
        "# Acessar o Preprocessor (ColumnTransformer + SimpleImputer) treinado\n",
        "preprocessor = pipeline_ml_classico.named_steps['columntransformer']\n",
        "\n",
        "# Transformar os dados de teste usando TODOS os passos de pré-processamento\n",
        "# Aplicar o ColumnTransformer/Imputer\n",
        "X_test_imputed = preprocessor.transform(X_test)\n",
        "\n",
        "# Aplicar o TableVectorizer aos dados imputados.\n",
        "# O Skrub agora recebe a matriz que ele esperava ter na etapa de fit!\n",
        "X_test_clean = vectorizer.transform(X_test_imputed)\n",
        "feature_names_clean = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Criar o SHAP Explainer\n",
        "# Usaremos o Explainer para modelos lineares\n",
        "explainer = shap.Explainer(model, X_test_clean)\n",
        "shap_values = explainer(X_test_clean)\n",
        "\n",
        "# Selecionar um cliente de ALTO RISCO (Exemplo para o Agente de IA)\n",
        "index_risco = np.where(y_test == 1)[0][0]\n",
        "cliente_risco_X = X_test.iloc[index_risco]\n",
        "\n",
        "# Para prever a probabilidade do cliente individual, ele deve passar pelo pipeline COMPLETO\n",
        "probabilidade_churn = pipeline_ml_classico.predict_proba(cliente_risco_X.to_frame().T)[:, 1][0]\n",
        "shap_local = shap_values[index_risco]\n",
        "\n",
        "print(\"\\n--- Resultado SHAP (Para o Raciocínio do Agente) ---\")\n",
        "print(f\"Probabilidade de Churn do Cliente Selecionado: {probabilidade_churn:.2f}\")\n",
        "\n",
        "# Mapear os valores SHAP para um formato legível pelo Agente (JSON)\n",
        "shap_df = pd.DataFrame({\n",
        "    'Feature': feature_names_clean,\n",
        "    'SHAP_Value': shap_local.values\n",
        "}).sort_values(by='SHAP_Value', ascending=False)\n",
        "\n",
        "# Fatores que mais contribuíram POSITIVAMENTE para o Churn (Top 3)\n",
        "top_churn_factors = shap_df.head(3).to_dict(orient='records')\n",
        "print(f\"Top 3 Fatores de Risco (SHAP): {top_churn_factors}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvPB0Mlr8mfr",
        "outputId": "0a4f6f34-099a-49f5-8435-176ce96be998"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/skrub/_check_input.py:175: UserWarning: Only pandas and polars DataFrames are supported, but input is a Numpy array. Please convert Numpy arrays to DataFrames before passing them to skrub transformers. Converting to pandas DataFrame with columns ['0', '1', …].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/skrub/_clean_null_strings.py:39: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  return col.replace(r\"^\\s*$\", \"\", regex=True)\n",
            "/usr/local/lib/python3.12/dist-packages/skrub/_check_input.py:175: UserWarning: Only pandas and polars DataFrames are supported, but input is a Numpy array. Please convert Numpy arrays to DataFrames before passing them to skrub transformers. Converting to pandas DataFrame with columns ['0', '1', …].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/skrub/_clean_null_strings.py:39: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  return col.replace(r\"^\\s*$\", \"\", regex=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Resultado SHAP (Para o Raciocínio do Agente) ---\n",
            "Probabilidade de Churn do Cliente Selecionado: 0.40\n",
            "Top 3 Fatores de Risco (SHAP): [{'Feature': '1', 'SHAP_Value': 0.7309180494810087}, {'Feature': '4_Month-to-month', 'SHAP_Value': 0.2857466390001965}, {'Feature': '11_Yes', 'SHAP_Value': 0.16723687034076076}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simulação do Agente de IA e Log de Auditabilidade\n",
        "Roteamento Híbrido (FinOps), o Raciocínio (LLM) e a Auditabilidade (Git)."
      ],
      "metadata": {
        "id": "msJJImfq99xV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GIT_HASH = hashlib.sha256(str(time.time()).encode()).hexdigest()[:8]\n",
        "LLM_VERSION = \"GPT-4o (Simulado)\"\n",
        "FINOPS_THRESHOLD = 0.70 # Acima de 70% de risco, aciona o Agente caro.\n",
        "\n",
        "# Lógica do Roteamento Híbrido (FinOps)\n",
        "if probabilidade_churn >= FINOPS_THRESHOLD:\n",
        "    print(f\"\\n--- Roteamento Híbrido ATIVADO: Risco ({probabilidade_churn:.2f}) > Limiar ({FINOPS_THRESHOLD:.2f}) ---\")\n",
        "\n",
        "    # Entrada de Dados Estruturados para o Agente (SHAP + Previsão)\n",
        "    contexto_para_agente = {\n",
        "        \"Probabilidade_Churn\": f\"{probabilidade_churn:.2f}\",\n",
        "        \"Fatores_Risco\": top_churn_factors\n",
        "    }\n",
        "\n",
        "    # Simulação do Raciocínio (LLM/Prompt Engineering)\n",
        "    # O LLM consumiria 'contexto_para_agente' e geraria esta Ação.\n",
        "    acao_sugerida = f\"O Cliente [ID: {cliente_risco_X.name}] tem um risco crítico de {probabilidade_churn:.0%}. A principal causa é o '{contexto_para_agente['Fatores_Risco'][0]['Feature']}'. Ação: Enviar OFERTA VIP + Ligação de Suporte de Nível 3.\"\n",
        "\n",
        "    # Log de Auditabilidade e FinOps\n",
        "    log_auditoria = {\n",
        "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        \"cliente_id\": cliente_risco_X.name,\n",
        "        \"previsao_ml_classico\": probabilidade_churn,\n",
        "        \"decisao_agente\": acao_sugerida,\n",
        "        \"modelo_ml_hash\": GIT_HASH,\n",
        "        \"llm_usado\": LLM_VERSION,\n",
        "        \"fatores_interpretacao_shap\": top_churn_factors\n",
        "    }\n",
        "\n",
        "    print(f\"Decisão do Agente: {acao_sugerida}\")\n",
        "    print(\"\\n--- Log de Auditoria Gerado (Json) ---\")\n",
        "    print(json.dumps(log_auditoria, indent=4))\n",
        "\n",
        "else:\n",
        "    # Cenário de Otimização FinOps: LLM não é ativado\n",
        "    print(f\"\\n--- Otimização FinOps: Risco ({probabilidade_churn:.2f}) Abaixo do Limiar ---\")\n",
        "    print(\"Ação: Nenhuma ação dispendiosa com LLM foi tomada.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgB3C5ex9hv6",
        "outputId": "b1f4281b-c676-4743-d978-3641760e6752"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Otimização FinOps: Risco (0.40) Abaixo do Limiar ---\n",
            "Ação: Nenhuma ação dispendiosa com LLM foi tomada.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Outra SIMULAÇÃO"
      ],
      "metadata": {
        "id": "I2BA1Eql_S3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup e Instalação do MLflow"
      ],
      "metadata": {
        "id": "VHTMJj53_ffT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalações adicionais necessárias\n",
        "!pip install mlflow\n",
        "import mlflow\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Configurar o MLflow para rastrear localmente (pode ser substituído por um servidor remoto)\n",
        "mlflow.set_tracking_uri(\"sqlite:///mlruns.db\")\n",
        "mlflow.set_experiment(\"churn_hybrid_agent_project\")\n",
        "\n",
        "# Simulação de variáveis de governança (Agora gerenciadas pelo MLflow/Sistema)\n",
        "FINOPS_THRESHOLD = 0.70\n",
        "LLM_PROVIDER = \"Gemini API (Simulado)\"\n",
        "\n",
        "# Função para simular a chamada da API do LLM\n",
        "def generate_agent_action(contexto_para_agente, cliente_id):\n",
        "    \"\"\"\n",
        "    Simula uma chamada real a um LLM (como Gemini, GPT, etc.) para raciocínio.\n",
        "    Em um projeto real, esta função faria uma requisição HTTP para a API.\n",
        "    \"\"\"\n",
        "    # Em um LLM real, você usaria o contexto_para_agente para construir um prompt:\n",
        "    # prompt = f\"O cliente {cliente_id} tem {contexto_para_agente['Probabilidade_Churn']} de churn.\n",
        "    # Os fatores de risco são: {contexto_para_agente['Fatores_Risco']}. Sugira uma ação tática de retenção.\"\n",
        "\n",
        "    prob_churn = float(contexto_para_agente['Probabilidade_Churn'])\n",
        "    fator_principal = contexto_para_agente['Fatores_Risco'][0]['Feature']\n",
        "\n",
        "    # Simulação do Raciocínio baseado no fator principal:\n",
        "    if \"MonthlyCharges\" in fator_principal or \"TotalCharges\" in fator_principal:\n",
        "        sugestao = \"Oferta de desconto de 15% para fidelidade e monitoramento do uso.\"\n",
        "    elif \"tenure\" in fator_principal:\n",
        "        sugestao = \"Ligação de Nível 3 para pesquisa de satisfação profunda e proposta de novo contrato.\"\n",
        "    else:\n",
        "        sugestao = \"E-mail com oferta de recursos premium gratuitos por um mês.\"\n",
        "\n",
        "    acao_sugerida = f\"O Cliente [ID: {cliente_id}] tem um risco crítico de {prob_churn:.0%}. A principal causa é o '{fator_principal}'. Ação: {sugestao}\"\n",
        "\n",
        "    return acao_sugerida"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abUphfAj_PBN",
        "outputId": "305b53dc-c654-4ed3-dd2a-38bc44200f46"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mlflow\n",
            "  Downloading mlflow-3.7.0-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting mlflow-skinny==3.7.0 (from mlflow)\n",
            "  Downloading mlflow_skinny-3.7.0-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting mlflow-tracing==3.7.0 (from mlflow)\n",
            "  Downloading mlflow_tracing-3.7.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting Flask-CORS<7 (from mlflow)\n",
            "  Downloading flask_cors-6.0.2-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.1.2)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.17.2)\n",
            "Requirement already satisfied: cryptography<47,>=43.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (43.0.3)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting graphene<4 (from mlflow)\n",
            "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting gunicorn<24 (from mlflow)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting huey<3,>=2.5.0 (from mlflow)\n",
            "  Downloading huey-2.5.5-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.0.2)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<23,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (18.1.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.6.1)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.16.3)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.0.45)\n",
            "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.7.0->mlflow) (6.2.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.7.0->mlflow) (8.3.1)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.7.0->mlflow) (3.1.2)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.7.0->mlflow)\n",
            "  Downloading databricks_sdk-0.74.0-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fastapi<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.7.0->mlflow) (0.118.3)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.7.0->mlflow) (3.1.45)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.7.0->mlflow) (8.7.0)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.7.0->mlflow) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-proto<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.7.0->mlflow) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.7.0->mlflow) (1.37.0)\n",
            "Requirement already satisfied: packaging<26 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.7.0->mlflow) (25.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.7.0->mlflow) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.7.0->mlflow) (2.12.3)\n",
            "Requirement already satisfied: python-dotenv<2,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.7.0->mlflow) (1.2.1)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.7.0->mlflow) (6.0.3)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.7.0->mlflow) (2.32.4)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.7.0->mlflow) (0.5.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.7.0->mlflow) (4.15.0)\n",
            "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.7.0->mlflow) (0.38.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.3.10)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography<47,>=43.0.0->mlflow) (2.0.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from docker<8,>=4.0.0->mlflow) (2.5.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.0.3)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.1.4)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_core-3.2.7-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from graphene<4->mlflow) (2.9.0.post0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (3.2.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2->mlflow) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2->mlflow) (3.6.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography<47,>=43.0.0->mlflow) (2.23)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.12/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.7.0->mlflow) (2.43.0)\n",
            "Requirement already satisfied: starlette<0.49.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi<1->mlflow-skinny==3.7.0->mlflow) (0.48.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.7.0->mlflow) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.7.0->mlflow) (3.23.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.7.0->mlflow) (0.58b0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.7.0->mlflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.7.0->mlflow) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.7.0->mlflow) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.7.0->mlflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.7.0->mlflow) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.7.0->mlflow) (2025.11.12)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn<1->mlflow-skinny==3.7.0->mlflow) (0.16.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.7.0->mlflow) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.7.0->mlflow) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.7.0->mlflow) (4.9.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.49.0,>=0.40.0->fastapi<1->mlflow-skinny==3.7.0->mlflow) (4.12.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.7.0->mlflow) (0.6.1)\n",
            "Downloading mlflow-3.7.0-py3-none-any.whl (8.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-3.7.0-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m107.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_tracing-3.7.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flask_cors-6.0.2-py3-none-any.whl (13 kB)\n",
            "Downloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huey-2.5.5-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.74.0-py3-none-any.whl (764 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m764.2/764.2 kB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.7-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: huey, gunicorn, graphql-core, graphql-relay, docker, graphene, Flask-CORS, databricks-sdk, mlflow-tracing, mlflow-skinny, mlflow\n",
            "Successfully installed Flask-CORS-6.0.2 databricks-sdk-0.74.0 docker-7.1.0 graphene-3.4.3 graphql-core-3.2.7 graphql-relay-3.2.0 gunicorn-23.0.0 huey-2.5.5 mlflow-3.7.0 mlflow-skinny-3.7.0 mlflow-tracing-3.7.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/12/12 23:56:59 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
            "2025/12/12 23:56:59 INFO mlflow.store.db.utils: Updating database tables\n",
            "2025/12/12 23:56:59 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
            "2025/12/12 23:56:59 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
            "2025/12/12 23:56:59 INFO alembic.runtime.migration: Running upgrade  -> 451aebb31d03, add metric step\n",
            "2025/12/12 23:56:59 INFO alembic.runtime.migration: Running upgrade 451aebb31d03 -> 90e64c465722, migrate user column to tags\n",
            "2025/12/12 23:56:59 INFO alembic.runtime.migration: Running upgrade 90e64c465722 -> 181f10493468, allow nulls for metric values\n",
            "2025/12/12 23:56:59 INFO alembic.runtime.migration: Running upgrade 181f10493468 -> df50e92ffc5e, Add Experiment Tags Table\n",
            "2025/12/12 23:56:59 INFO alembic.runtime.migration: Running upgrade df50e92ffc5e -> 7ac759974ad8, Update run tags with larger limit\n",
            "2025/12/12 23:56:59 INFO alembic.runtime.migration: Running upgrade 7ac759974ad8 -> 89d4b8295536, create latest metrics table\n",
            "2025/12/12 23:56:59 INFO alembic.runtime.migration: Running upgrade 89d4b8295536 -> 2b4d017a5e9b, add model registry tables to db\n",
            "2025/12/12 23:56:59 INFO alembic.runtime.migration: Running upgrade 2b4d017a5e9b -> cfd24bdc0731, Update run status constraint with killed\n",
            "2025/12/12 23:56:59 INFO alembic.runtime.migration: Running upgrade cfd24bdc0731 -> 0a8213491aaa, drop_duplicate_killed_constraint\n",
            "2025/12/12 23:56:59 INFO alembic.runtime.migration: Running upgrade 0a8213491aaa -> 728d730b5ebd, add registered model tags table\n",
            "2025/12/12 23:56:59 INFO alembic.runtime.migration: Running upgrade 728d730b5ebd -> 27a6a02d2cf1, add model version tags table\n",
            "2025/12/12 23:56:59 INFO alembic.runtime.migration: Running upgrade 27a6a02d2cf1 -> 84291f40a231, add run_link to model_version\n",
            "2025/12/12 23:56:59 INFO alembic.runtime.migration: Running upgrade 84291f40a231 -> a8c4a736bde6, allow nulls for run_id\n",
            "2025/12/12 23:56:59 INFO alembic.runtime.migration: Running upgrade a8c4a736bde6 -> 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary\n",
            "2025/12/12 23:56:59 INFO alembic.runtime.migration: Running upgrade 39d1c3be5f05 -> c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql\n",
            "2025/12/12 23:56:59 INFO alembic.runtime.migration: Running upgrade c48cb773bb87 -> bd07f7e963c5, create index on run_uuid\n",
            "2025/12/12 23:56:59 INFO alembic.runtime.migration: Running upgrade bd07f7e963c5 -> 0c779009ac13, add deleted_time field to runs table\n",
            "2025/12/12 23:56:59 INFO alembic.runtime.migration: Running upgrade 0c779009ac13 -> cc1f77228345, change param value length to 500\n",
            "2025/12/12 23:56:59 INFO alembic.runtime.migration: Running upgrade cc1f77228345 -> 97727af70f4d, Add creation_time and last_update_time to experiments table\n",
            "2025/12/12 23:56:59 INFO alembic.runtime.migration: Running upgrade 97727af70f4d -> 3500859a5d39, Add Model Aliases table\n",
            "2025/12/12 23:56:59 INFO alembic.runtime.migration: Running upgrade 3500859a5d39 -> 7f2a7d5fae7d, add datasets inputs input_tags tables\n",
            "2025/12/12 23:56:59 INFO alembic.runtime.migration: Running upgrade 7f2a7d5fae7d -> 2d6e25af4d3e, increase max param val length from 500 to 8000\n",
            "2025/12/12 23:56:59 INFO alembic.runtime.migration: Running upgrade 2d6e25af4d3e -> acf3f17fdcc7, add storage location field to model versions\n",
            "2025/12/12 23:56:59 INFO alembic.runtime.migration: Running upgrade acf3f17fdcc7 -> 867495a8f9d4, add trace tables\n",
            "2025/12/12 23:56:59 INFO alembic.runtime.migration: Running upgrade 867495a8f9d4 -> 5b0e9adcef9c, add cascade deletion to trace tables foreign keys\n",
            "2025/12/12 23:56:59 INFO alembic.runtime.migration: Running upgrade 5b0e9adcef9c -> 4465047574b1, increase max dataset schema size\n",
            "2025/12/12 23:56:59 INFO alembic.runtime.migration: Running upgrade 4465047574b1 -> f5a4f2784254, increase run tag value limit to 8000\n",
            "2025/12/12 23:56:59 INFO alembic.runtime.migration: Running upgrade f5a4f2784254 -> 0584bdc529eb, add cascading deletion to datasets from experiments\n",
            "2025/12/12 23:56:59 INFO alembic.runtime.migration: Running upgrade 0584bdc529eb -> 400f98739977, add logged model tables\n",
            "2025/12/12 23:57:00 INFO alembic.runtime.migration: Running upgrade 400f98739977 -> 6953534de441, add step to inputs table\n",
            "2025/12/12 23:57:00 INFO alembic.runtime.migration: Running upgrade 6953534de441 -> bda7b8c39065, increase_model_version_tag_value_limit\n",
            "2025/12/12 23:57:00 INFO alembic.runtime.migration: Running upgrade bda7b8c39065 -> cbc13b556ace, add V3 trace schema columns\n",
            "2025/12/12 23:57:00 INFO alembic.runtime.migration: Running upgrade cbc13b556ace -> 770bee3ae1dd, add assessments table\n",
            "2025/12/12 23:57:00 INFO alembic.runtime.migration: Running upgrade 770bee3ae1dd -> a1b2c3d4e5f6, add spans table\n",
            "2025/12/12 23:57:00 INFO alembic.runtime.migration: Running upgrade a1b2c3d4e5f6 -> de4033877273, create entity_associations table\n",
            "2025/12/12 23:57:00 INFO alembic.runtime.migration: Running upgrade de4033877273 -> 1a0cddfcaa16, Add webhooks and webhook_events tables\n",
            "2025/12/12 23:57:00 INFO alembic.runtime.migration: Running upgrade 1a0cddfcaa16 -> 534353b11cbc, add scorer tables\n",
            "2025/12/12 23:57:00 INFO alembic.runtime.migration: Running upgrade 534353b11cbc -> 71994744cf8e, add evaluation datasets\n",
            "2025/12/12 23:57:00 INFO alembic.runtime.migration: Running upgrade 71994744cf8e -> 3da73c924c2f, add outputs to dataset record\n",
            "2025/12/12 23:57:00 INFO alembic.runtime.migration: Running upgrade 3da73c924c2f -> bf29a5ff90ea, add jobs table\n",
            "2025/12/12 23:57:00 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
            "2025/12/12 23:57:00 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
            "2025/12/12 23:57:00 INFO mlflow.tracking.fluent: Experiment with name 'churn_hybrid_agent_project' does not exist. Creating a new experiment.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rastreamento do Treinamento (MLflow)"
      ],
      "metadata": {
        "id": "1rIB8f96AQGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Início do Rastreamento MLflow\n",
        "with mlflow.start_run() as run:\n",
        "\n",
        "    # Parâmetros de Governança\n",
        "    mlflow.log_param(\"finops_threshold\", FINOPS_THRESHOLD)\n",
        "    mlflow.log_param(\"modelo_ml_algoritmo\", \"LogisticRegression\")\n",
        "\n",
        "    # Treinar o Pipeline (Skrub + ML Clássico)\n",
        "    # Assume-se que pipeline_ml_classico, X_train, y_train já estão definidos\n",
        "    pipeline_ml_classico.fit(X_train, y_train)\n",
        "\n",
        "    # Avaliar e Logar Métricas (ML Clássico)\n",
        "    y_pred = pipeline_ml_classico.predict(X_test)\n",
        "    report = classification_report(y_test, y_pred, output_dict=True)\n",
        "    mlflow.log_metric(\"accuracy\", report['accuracy'])\n",
        "    mlflow.log_metric(\"f1_score_churn\", report['1']['f1-score'])\n",
        "\n",
        "    # Registrar o Modelo (para rastreabilidade do Skrub + RegLog)\n",
        "    # MLflow rastreia automaticamente o ambiente e o código que criou o modelo.\n",
        "    mlflow.sklearn.log_model(\n",
        "        sk_model=pipeline_ml_classico,\n",
        "        artifact_path=\"churn_model_pipeline\",\n",
        "        registered_model_name=\"Skrub_Hybrid_Churn_Predictor\"\n",
        "    )\n",
        "\n",
        "    RUN_ID = run.info.run_id\n",
        "    print(f\"\\n--- MLflow Treinamento Concluído ---\")\n",
        "    print(f\"Modelo registrado com RUN_ID: {RUN_ID}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MXnJEx1_tRs",
        "outputId": "719bb906-685b-44cb-ddc5-237dda11136e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/skrub/_check_input.py:175: UserWarning: Only pandas and polars DataFrames are supported, but input is a Numpy array. Please convert Numpy arrays to DataFrames before passing them to skrub transformers. Converting to pandas DataFrame with columns ['0', '1', …].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/skrub/_clean_null_strings.py:39: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  return col.replace(r\"^\\s*$\", \"\", regex=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/skrub/_check_input.py:175: UserWarning: Only pandas and polars DataFrames are supported, but input is a Numpy array. Please convert Numpy arrays to DataFrames before passing them to skrub transformers. Converting to pandas DataFrame with columns ['0', '1', …].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/skrub/_clean_null_strings.py:39: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  return col.replace(r\"^\\s*$\", \"\", regex=True)\n",
            "2025/12/13 00:00:08 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/12/13 00:00:14 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
            "2025/12/13 00:00:14 INFO mlflow.store.db.utils: Updating database tables\n",
            "2025/12/13 00:00:14 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
            "2025/12/13 00:00:14 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- MLflow Treinamento Concluído ---\n",
            "Modelo registrado com RUN_ID: fa0b71b6e1254fd3938ad67bff3b6ad2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Successfully registered model 'Skrub_Hybrid_Churn_Predictor'.\n",
            "Created version '1' of model 'Skrub_Hybrid_Churn_Predictor'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Integração Híbrida e Log de Auditoria Real\n",
        "Usar a função generate_agent_action (simulação do LLM) e garantir que o log de auditoria capture o ID do MLflow para a rastreabilidade."
      ],
      "metadata": {
        "id": "JC5lw1CXApVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lógica do Roteamento Híbrido (FinOps)\n",
        "if probabilidade_churn >= FINOPS_THRESHOLD:\n",
        "\n",
        "    # Entrada de Dados Estruturados (SHAP + Previsão)\n",
        "    contexto_para_agente = {\n",
        "        \"Probabilidade_Churn\": f\"{probabilidade_churn:.2f}\",\n",
        "        \"Fatores_Risco\": top_churn_factors\n",
        "    }\n",
        "\n",
        "    # CHAMADA REAL (Simulada) AO LLM\n",
        "    acao_sugerida = generate_agent_action(contexto_para_agente, cliente_risco_X.name)\n",
        "\n",
        "    # Log de Auditabilidade e FinOps\n",
        "    log_auditoria = {\n",
        "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        \"cliente_id\": cliente_risco_X.name,\n",
        "        \"previsao_ml_classico\": probabilidade_churn,\n",
        "        \"decisao_agente\": acao_sugerida,\n",
        "        \"mlflow_run_id\": RUN_ID,\n",
        "        \"llm_provider\": LLM_PROVIDER,\n",
        "        \"fatores_interpretacao_shap\": top_churn_factors\n",
        "    }\n",
        "\n",
        "    print(f\"\\n--- Roteamento Híbrido ATIVADO: (Risco: {probabilidade_churn:.2f}) ---\")\n",
        "    print(f\"Decisão do Agente (LLM): {acao_sugerida}\")\n",
        "    print(\"\\n--- Log de Auditoria Final (Para BD de Log) ---\")\n",
        "    print(json.dumps(log_auditoria, indent=4))\n",
        "\n",
        "\n",
        "    print(\"\\n[Ação Simulado]: Enviado para o sistema de CRM para execução!\")\n",
        "\n",
        "else:\n",
        "    # Cenário de Otimização FinOps:\n",
        "    print(f\"\\n--- Otimização FinOps: Risco ({probabilidade_churn:.2f}) Abaixo do Limiar ---\")\n",
        "    print(\"Ação: Nenhuma chamada dispendiosa ao LLM foi realizada.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8UjmNoKAequ",
        "outputId": "c3dfd8f0-edad-4de7-ac9a-0c09a4469491"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Otimização FinOps: Risco (0.40) Abaixo do Limiar ---\n",
            "Ação: Nenhuma chamada dispendiosa ao LLM foi realizada.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rastreabilidade MLflow: Agora, se a Decisão do Agente estiver errada, você tem o mlflow_run_id para consultar o MLflow e ver a versão exata do código Skrub e do modelo de ML Clássico usado.\n",
        "\n",
        "### Integração LLM: A função generate_agent_action serve como interface clara, pronta para ser trocada por uma chamada requests.post à API do Gemini ou outro LLM, transformando o contexto_para_agente (os dados SHAP) em um prompt estruturado."
      ],
      "metadata": {
        "id": "sqoXkvn2BcEu"
      }
    }
  ]
}